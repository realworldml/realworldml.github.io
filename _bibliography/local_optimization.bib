@inproceedings{HennigKiefel,
  author =	 {P. Hennig and M. Kiefel},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Quasi-{N}ewton methods -- a new direction}},
  year =	 2012,
  abstract =	 {Four decades after their invention, quasi- Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Al- though not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of clas- sical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  video =
                  {http://techtalks.tv/talks/quasi-newton-methods-a-new-direction/57289/},
  file =	 {../assets/pdf/hennig13quasiNewton.pdf}
}

@article{hennig13_quasi_newton_method,
  author =	 {P. Hennig and M. Kiefel},
  journal =	 {Journal of Machine Learning Research},
  month =	 {March},
  pages =	 {834--865},
  title =	 {Quasi-{N}ewton Methods -- a new direction},
  volume =	 14,
  year =	 2013,
  abstract =	 {Four decades after their invention, quasi-Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Although not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of classical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.},
  file = {http://jmlr.org/papers/volume14/hennig13a/hennig13a.pdf}
}

@inproceedings{StochasticNewton,
  author =	 {P. Hennig},
  booktitle =	 {{International Conference on Machine Learning (ICML)}},
  title =	 {{Fast Probabilistic Optimization from Noisy Gradients}},
  year =	 {2013},
  abstract =	 {Stochastic gradient descent remains popular in large-scale
                  machine learning, on account of its very low computational
                  cost and robust- ness to noise. However, gradient descent is
                  only linearly efficient and not transformation
                  invariant. Scaling by a local measure can substantially
                  improve its performance. One natural choice of such a scale
                  is the Hessian of the objective function: Were it available,
                  it would turn linearly efficient gradient descent into the
                  quadratically efficient Newton-Raphson optimization. Existing
                  covariant methods, though, are either super-linearly
                  expensive or do not address noise. Generalising recent
                  results, this paper constructs a nonparametric Bayesian
                  quasi-Newton algorithm that learns gradient and Hessian from
                  noisy evaluations of the gradient. Importantly, the resulting
                  algorithm, like stochastic gradient descent, has cost linear
                  in the number of input dimensions},
  file = {../assets/pdf/hennig13noisy.pdf}
}

@incollection{NIPS2015_5753,
title = {Probabilistic Line Searches for Stochastic Optimization},
author = {Mahsereci, Maren and Hennig, Philipp},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {181--189},
year = {2015},
publisher = {Curran Associates, Inc.},
file = {http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization.pdf},
abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
code = {https://is.tuebingen.mpg.de/uploads_file/attachment/attachment/242/probLS.zip},
link = {http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization}
}

@ARTICLE{2018arXiv180704594D,
   author = {{Deniz Akyildiz}, {\"O}. and {Elvira}, V. and {Miguez}, J.},
    title = "{The Incremental Proximal Method: A Probabilistic Perspective}",
  journal = {ArXiv e-prints},
   volume = {1807.04594},
     year = 2018,
    month = jul,
   abstract = {In this work, we highlight a connection between the incremental proximal method and stochastic filters. We begin by showing that the proximal operators coincide, and hence can be realized with, Bayes updates. We give the explicit form of the updates for the linear regression problem and show that there is a one-to-one correspondence between the proximal operator of the least-squares regression and the Bayes update when the prior and the likelihood are Gaussian. We then carry out this observation to a general sequential setting: We consider the incremental proximal method, which is an algorithm for large-scale optimization, and show that, for a linear-quadratic cost function, it can naturally be realized by the Kalman filter. We then discuss the implications of this idea for nonlinear optimization problems where proximal operators are in general not realizable. In such settings, we argue that the extended Kalman filter can provide a systematic way for the derivation of practical procedures.},
   link = {https://arxiv.org/abs/1807.04594},
   file = {https://arxiv.org/pdf/1807.04594.pdf}
}
